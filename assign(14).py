# -*- coding: utf-8 -*-
"""Assign(14).ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1SHJXlD7tSazLSYMZi-zJs9XhN052JKzU
"""

import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
import warnings
warnings.filterwarnings('ignore')
from sklearn.preprocessing import StandardScaler
from sklearn.decomposition import PCA
from sklearn.cluster import KMeans
from sklearn.metrics import silhouette_score

"""Task 1: Exploratory Data Analysis (EDA):"""

# Load the dataset
df = pd.read_csv('wine.csv')

# Basic exploration
print(df.info())
print(df.describe())

# Examine distribution of features (Example: Alcohol)
sns.histplot(df['Alcohol'], kde=True)
plt.title('Distribution of Alcohol Content')
plt.show()

# Investigate correlations
plt.figure(figsize=(10, 8))
sns.heatmap(df.corr(), annot=True, fmt='.2f')
plt.title('Correlation Matrix')
plt.show()

"""Task 2: Dimensionality Reduction with PCA:"""

# Standardize features
# Dropping 'Type' as it is the target label, not a feature
X = df.drop('Type', axis=1)
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# Implement PCA
pca = PCA()
pca_values = pca.fit_transform(X_scaled)

# Determine optimal number of components using Scree Plot
var = pca.explained_variance_ratio_
cum_var = np.cumsum(np.round(var, decimals=4) * 100)

plt.plot(cum_var, marker='o', linestyle='--', color='b')
plt.title('Scree Plot: Cumulative Explained Variance')
plt.xlabel('Number of Components')
plt.ylabel('Variance Explained (%)')
plt.show()

# Transform dataset into the top 3 principal components
pca_final = PCA(n_components=3)
pca_data = pca_final.fit_transform(X_scaled)
pca_df = pd.DataFrame(pca_data, columns=['PC1', 'PC2', 'PC3'])

"""Task 3: Clustering with Original Data:"""

# Apply K-means (assuming 3 clusters based on wine types)
kmeans_orig = KMeans(n_clusters=3, random_state=42)
clusters_orig = kmeans_orig.fit_predict(X_scaled)

# Evaluate performance
score_orig = silhouette_score(X_scaled, clusters_orig)
print(f"Silhouette Score (Original Data): {score_orig:.4f}")

"""Task 4: Clustering with PCA Data:"""

# Apply K-means to PCA-transformed data
kmeans_pca = KMeans(n_clusters=3, random_state=42)
clusters_pca = kmeans_pca.fit_predict(pca_df)

# Visualize PCA clustering results
plt.scatter(pca_df['PC1'], pca_df['PC2'], c=clusters_pca, cmap='viridis')
plt.title('Clustering Visualization (First 2 Principal Components)')
plt.xlabel('PC1')
plt.ylabel('PC2')
plt.show()

# Evaluate performance
score_pca = silhouette_score(pca_df, clusters_pca)
print(f"Silhouette Score (PCA Data): {score_pca:.4f}")

"""Task 5: Comparison and Analysis:

Comparing the two approaches reveals the impact of dimensionality reduction:

- Similarities: Both methods often identify the same primary clusters in the Wine dataset.
- Differences: Clustering on PCA data often results in a higher Silhouette Score because PCA removes "noise" (less significant variance), making the clusters more distinct in the reduced space.
- Trade-offs: While PCA makes computation faster and improves visualization, it does involve a slight loss of information from the discarded components.

Task 6: Conclusion and Insights:

- Key Findings: PCA successfully reduced 13 features into 3 components while retaining the majority of the variance.

- Practical Implications: Dimensionality reduction is vital for high-dimensional datasets to overcome the "curse of dimensionality" and improve the efficiency of machine learning models.

- Recommendations: Use PCA before clustering when dealing with many correlated features or when visualization of high-dimensional data is required. Perform clustering on original data only when the number of features is small and every variable is known to be critical for the outcome.
"""