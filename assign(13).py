# -*- coding: utf-8 -*-
"""Assign(13).ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1uXBAJ6yiOv7QpoiH50gqndI2wHn-X0RB
"""

import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from xgboost import XGBClassifier
from lightgbm import LGBMClassifier
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score
import warnings
warnings.filterwarnings('ignore')

"""Exploratory Data Analysis (EDA):"""

# 1. Load the diabetes dataset
df = pd.read_csv('diabetes.csv')

# 2. Check for missing values
print("Missing Values per Column:\n", df.isnull().sum())

# 3. Explore data distributions using histograms
df.hist(bins=20, figsize=(15, 10))
plt.suptitle("Feature Distributions")
plt.show()

# 4. Visualize relationships (Correlation Heatmap)
plt.figure(figsize=(10, 8))
sns.heatmap(df.corr(), annot=True)
plt.title("Feature Correlations")
plt.show()

"""Data Preprocessing:"""

# Many 0 values in this dataset act as missing values (e.g., BloodPressure)
# 1. Impute missing values (Replacing 0 with median for specific columns)
cols_with_zeros = ['Glucose', 'BloodPressure', 'SkinThickness', 'Insulin', 'BMI']
for col in cols_with_zeros:
    df[col] = df[col].replace(0, df[col].median())

# 2. Split into features (X) and target (y)
X = df.drop('Outcome', axis=1)
y = df['Outcome']

# 3. Scale features for better model performance
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# 4. Split into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)

"""Building Predictive Models:"""

# 1. Build and Train XGBoost
xgb_model = XGBClassifier(use_label_encoder=False, eval_metric='logloss', random_state=42)
xgb_model.fit(X_train, y_train)
xgb_preds = xgb_model.predict(X_test)

# 2. Build and Train LightGBM
lgbm_model = LGBMClassifier(random_state=42)
lgbm_model.fit(X_train, y_train)
lgbm_preds = lgbm_model.predict(X_test)

print("Models trained successfully!")

"""Comparative Analysis:"""

def evaluate_model(y_true, y_pred, name):
    return {
        'Model': name,
        'Accuracy': accuracy_score(y_true, y_pred),
        'Precision': precision_score(y_true, y_pred),
        'Recall': recall_score(y_true, y_pred),
        'F1-Score': f1_score(y_true, y_pred)
    }

# Collect results
results = pd.DataFrame([
    evaluate_model(y_test, xgb_preds, "XGBoost"),
    evaluate_model(y_test, lgbm_preds, "LightGBM")
])

print(results)

# Visualize Comparison
results.set_index('Model').plot(kind='bar', figsize=(10, 6))
plt.title("Comparative Analysis: XGBoost vs LightGBM")
plt.ylabel("Score")
plt.xticks(rotation=0)
plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')
plt.show()

"""Summary Report:

- Performance: On the Diabetes dataset, both models typically achieve similar accuracy (often around 74-78%).


- Speed: LightGBM is generally faster and more memory-efficient because it uses leaf-wise growth, whereas XGBoost uses level-wise growth.

- Conclusion: While XGBoost is highly robust, LightGBM is often preferred for larger datasets due to its efficiency. For this smaller diabetes dataset, the performance difference is marginal, and both are effective for binary classification.
"""