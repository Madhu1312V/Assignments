# -*- coding: utf-8 -*-
"""Assign(10).ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1SBCgXa926d8sLzaTcaJJLoMGU5b4PXZL
"""

import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.svm import SVC
from sklearn.decomposition import PCA
from sklearn.metrics import classification_report, confusion_matrix

"""Task 1: Exploratory Data Analysis (EDA)"""

data = pd.read_csv("Pharma_Industry.csv")

# Basic info
print(data.head())
print(data.info())
print(data.describe())

# Class distribution
print(data['Drug Response'].value_counts())
sns.countplot(x='Drug Response', data=data)
plt.title("Drug Response Distribution")
plt.show()

"""Visualizes class balance (0 = No response, 1 = Positive response)"""

# Correlation heatmap
plt.figure(figsize=(8,6))
sns.heatmap(data.corr(), annot=True)
plt.title("Feature Correlation Heatmap")
plt.show()

"""Heatmap shows relationships between biomarkers and drug response.

Task 2: Data Preprocessing

Prepare data for modeling.
"""

# Features and target
X = data.drop("Drug Response", axis=1)
y = data["Drug Response"]

# Train-test split
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=50, stratify=y
)

"""Splits dataset into training (80%) and testing (20%).

"""

# Feature scaling
std_sca = StandardScaler()
X_train = std_sca.fit_transform(X_train)
X_test = std_sca.transform(X_test)

"""Task 3: Data Visualization

Visualize feature distributions and relationships.
"""

# Pairplot for feature relationships
sns.pairplot(data, hue="Drug Response")
plt.show()

# Boxplot for glucose levels vs response
sns.boxplot(x="Drug Response", y="Blood Glucose Level (mg/dL)", data=data)
plt.title("Glucose Levels vs Drug Response")
plt.show()

"""Boxplot highlights biomarker differences between responders and non-responders.

Task 4: SVM Implementation

Train and evaluate a Support Vector Machine classifier.
"""

# Train SVM with RBF kernel
svm_model = SVC(kernel='rbf', C=1, gamma='scale')
svm_model.fit(X_train, y_train)

# Predictions
y_pred = svm_model.predict(X_test)

# Evaluation
print("Confusion Matrix:\n", confusion_matrix(y_test, y_pred))
print("\nClassification Report:\n", classification_report(y_test, y_pred))

"""Task 5: Visualization of SVM Results.

Visualize classification outcomes.
"""

# Confusion matrix heatmap
sns.heatmap(confusion_matrix(y_test, y_pred), annot=True, fmt="d")
plt.title("Confusion Matrix Heatmap")
plt.xlabel("Predicted")
plt.ylabel("Actual")
plt.show()

"""Heatmap shows prediction accuracy."""

# PCA-based decision boundary visualization
pca = PCA(n_components=2)
X_train_pca = pca.fit_transform(X_train)
X_test_pca = pca.transform(X_test)

svm_model_pca = SVC(kernel='rbf', C=1, gamma='scale')
svm_model_pca.fit(X_train_pca, y_train)

sns.scatterplot(x=X_test_pca[:,0], y=X_test_pca[:,1], hue=y_test, style=y_pred, palette="Set1")
plt.title("SVM Decision Boundary (PCA-reduced features)")
plt.show()

"""PCA plot illustrates how SVM separates classes in reduced dimensions.

Task 6: Parameter Tuning and Optimization

Best hyperparameters for SVM.
"""

from sklearn.model_selection import GridSearchCV

param_grid = {
    'C': [0.1, 1, 10],
    'gamma': ['scale', 0.1, 0.01],
    'kernel': ['linear', 'rbf', 'poly']
}

grid = GridSearchCV(SVC(), param_grid, refit=True, cv=5, verbose=1)
grid.fit(X_train, y_train)

print("Best Parameters:", grid.best_params_)
print("Best Cross-Validation Score:", grid.best_score_)

"""Task 7: Comparison and Analysis

Compare kernels, analyze strengths/weaknesses, and discuss real-world implications.
"""

kernels = ['linear', 'poly', 'rbf']
for k in kernels:
    model = SVC(kernel=k, C=1, gamma='scale')
    model.fit(X_train, y_train)
    y_pred_k = model.predict(X_test)
    print(f"\nKernel: {k}")
    print(classification_report(y_test, y_pred_k))

"""Analysis
- Linear kernel: Fast, interpretable, but limited for complex biological data.
- Polynomial kernel: Captures feature interactions but risks overfitting.
- RBF kernel: Best for non-linear biomarker relationships, usually highest accuracy.

Strengths of SVM
- Handles high-dimensional biomarker data.
- Effective in non-linear classification.
- Robust with proper tuning.

Weaknesses of SVM
- Sensitive to parameter choices.
- Harder to interpret compared to logistic regression.
- Computationally heavy for very large datasets.

Practical Implications
- Pharma Trials: Predict responders early, saving cost/time.
- Personalized Medicine: Tailor drugs to patient biomarkers.
- Healthcare AI: Works well with small but complex datasets.
- Regulatory Use: Supports evidence for drug approval.

"""