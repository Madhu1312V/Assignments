# -*- coding: utf-8 -*-
"""Assign(11).ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1jGWOwzAWwufO_G3pMqq0yMbzCdUKVp_J
"""

import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.tree import DecisionTreeClassifier
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix
from sklearn.model_selection import GridSearchCV
from sklearn import tree

"""1. Data Preparation"""

# Load the correct sheet by name or index
df = pd.read_excel("heart_disease.xlsx", sheet_name="Heart_disease")

# Check columns again
print(df.columns.tolist())

# Display first 5 rows
print(df.head())

# Check basic info
print(df.info())
print(df.describe())

df.head()

"""2. Exploratory Data Analysis (EDA)"""

# Strip whitespace from column names
df.columns = df.columns.str.strip()

print(df.columns.tolist())

print(df.select_dtypes(include='object').columns)

# Check missing values
print(df.isnull().sum())

# Histogram of age
sns.histplot(df['age'], bins=20, kde=True)
plt.show()

# Boxplot for cholesterol
sns.boxplot(x=df['chol'])
plt.show()

# Correlation heatmap
plt.figure(figsize=(10,8))
sns.heatmap(df_encoded.corr(), annot=True)
plt.show()

"""3. Feature Engineering"""

# Encode categorical variables
df_encoded = pd.get_dummies(df, drop_first=True)

# Define features and target
X = df_encoded.drop("num", axis=1)
y = df_encoded["num"]

# Convert target to binary (0 = no disease, 1 = disease)
y = np.where(y==0, 0, 1)

"""- pd.get_dummies() converts categorical variables into numeric (One-hot encoding).
- We separate features (X) and target (y).
- Target simplified into binary classification (disease vs no disease)

4. Decision Tree Classification
"""

# Split dataset
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Train Decision Tree
dt = DecisionTreeClassifier(random_state=42)
dt.fit(X_train, y_train)

# Predictions
y_pred = dt.predict(X_test)

# Evaluation
print("Accuracy:", accuracy_score(y_test, y_pred))
print("Classification Report:\n", classification_report(y_test, y_pred))
print("Confusion Matrix:\n", confusion_matrix(y_test, y_pred))

"""5. Hyperparameter Tuning"""

# Define parameter grid
param_grid = {
    'max_depth': [3, 5, 7, None],
    'min_samples_split': [2, 5, 10],
    'criterion': ['gini', 'entropy']
}

# Grid Search
grid = GridSearchCV(DecisionTreeClassifier(random_state=42), param_grid, cv=5)
grid.fit(X_train, y_train)

print("Best Parameters:", grid.best_params_)
best_model = grid.best_estimator_

# Evaluate best model
y_pred_best = best_model.predict(X_test)
print("Accuracy (Best Model):", accuracy_score(y_test, y_pred_best))

"""6. Model Evaluation and Visualization"""

# Visualize decision tree
plt.figure(figsize=(15,10))
tree.plot_tree(best_model, filled=True, feature_names=X.columns, class_names=["No Disease","Disease"])
plt.show()

# Feature importance
importances = best_model.feature_importances_
feat_importance = pd.Series(importances, index=X.columns).sort_values(ascending=False)
print(feat_importance.head(10))

"""Interview Questions:

1. Common hyperparameters of Decision Tree models and their effects:

- max_depth: Limits tree depth. Prevents overfitting if set small.
- min_samples_split: Minimum samples required to split a node. Larger values â†’ simpler tree.
- min_samples_leaf: Minimum samples in a leaf node. Helps smooth predictions.
- criterion: Metric to measure split quality (gini or entropy). Both affect how splits are chosen.
- max_features: Number of features considered at each split. Controls randomness.

2. Difference between Label Encoding and One-Hot Encoding:

- Label Encoding: Converts categories into integers (e.g., Male=1, Female=0). Good for ordinal data but may mislead models if categories are nominal.
- One-Hot Encoding: Creates binary columns for each category (e.g., Male=[1,0], Female=[0,1]). Prevents false ordering but increases dimensionality.



"""

