# -*- coding: utf-8 -*-
"""Assign(5).ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1TzXprrUUTpRCx8uPlTSCbVpU6Xg_7YdB
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import warnings
warnings.filterwarnings('ignore')

"""1.	Data Cleaning and Preparation:
●	Load the dataset into a data frame or equivalent data structure.
●	Handle missing values appropriately (e.g., imputation, deletion).
●	Identify and correct any inconsistencies in data types (e.g., numerical values stored as strings).
●	Detect and treat outliers if necessary.

"""

df=pd.read_csv('Cardiotocographic.csv')

df.shape

df.head()

df.info()

df.columns

df.isnull().sum()

df.isnull().sum().sum()

df.isnull().sum().sum()/len(df)*100

"""here missing values are more than 5%, so we can't drop them and we'll replace them with mean/median/mode(for numeric)"""

df.dtypes

df.fillna(df.median(),inplace=True)

df.isnull().sum().sum()

df.boxplot()
plt.show()

def outlier_cappping(df,column):
    Q1=df[column].quantile(0.25)
    Q3=df[column].quantile(0.75)
    IQR=Q3-Q1
    lower_extreme=Q1-1.5*IQR
    upper_extreme=Q3+1.5*IQR
    df[column]=df[column].apply(lambda x: lower_extreme if x<lower_extreme else upper_extreme if x>upper_extreme else x)
for col in df.select_dtypes(['int','float']).columns:
    outlier_cappping(df,col)

df.boxplot()
plt.show()

"""2.	Statistical Summary:
●	Provide a statistical summary for each variable in the dataset, including measures of central tendency (mean, median) and dispersion (standard deviation, interquartile range).
●	Highlight any interesting findings from this summary.

"""

df.describe()

cols=df.columns

summary=pd.DataFrame({
    'Mean': df[cols].mean(),
    'Median': df[cols].median(),
    'std': df[cols].std(),
    'Q1': df[cols].quantile(0.25),
    'Q3': df[cols].quantile(0.75),
    'IQR': df[cols].quantile(0.75)-df[cols].quantile(0.25)
    })

summary

"""3.	Data Visualization:
●	Create histograms or boxplots to visualize the distributions of various numerical variables.
●	Use bar charts or pie charts to display the frequency of categories for categorical variables.
●	Generate scatter plots or correlation heatmaps to explore relationships between pairs of variables.
●	Employ advanced visualization techniques like pair plots, or violin plots for deeper insights.

"""

df[cols].hist(figsize=(15,10),bins=30)
plt.tight_layout()
plt.show()

sns.boxplot(data=df[cols])
plt.figure(figsize=(15,10))
plt.tight_layout()
plt.show()

#NSP as categorical
df['NSP_cat'] = df['NSP'].round().astype(int)

df.head()

nsp_counts = df['NSP_cat'].value_counts().sort_index()
print('NSP class counts:\n', nsp_counts)

plt.figure(figsize=(6,4))
plt.bar(nsp_counts.index.astype(str), nsp_counts.values, color='skyblue')
plt.xlabel('NSP class')
plt.ylabel('Number of records')
plt.title('Bar chart of NSP classes')
plt.show()

plt.figure(figsize=(10, 8))
corr = df[cols].corr()
sns.heatmap(corr,annot=False)
plt.title('Correlation heatmap')
plt.show()

key_features = ['LB', 'ASTV', 'MSTV', 'ALTV', 'MLTV', 'Width', 'NSP_cat']
sns.pairplot(df[key_features], hue='NSP_cat', corner=True, diag_kind='hist')
plt.suptitle('Pairplot: Key features colored by NSP class', y=1)
plt.show()

"""4.	Pattern Recognition and Insights:
●	Identify any correlations between variables and discuss their potential implications.
●	Look for trends or patterns over time if temporal data is available.

"""

print('*** KEY PATTERNS FOUND ***')

#CORRELATIONS with NSP
corr_nsp = df.corr()['NSP'].sort_values(ascending=False)
print('\nTop 8 features correlated with NSP (fetal risk):')
print(corr_nsp.head(8).round(3))

# Explanation: ASTV,ALTV,MLTV-fetal risk indicators
#NSP class breakdown
print('\nNSP Class Distribution:')
print(df['NSP_cat'].value_counts().sort_index())
print('\nClass imbalance: 75%+ are normal (NSP=1)')

#Average values by NSP class (simple table)
nsp_stats = df.groupby('NSP_cat')[['ASTV','ALTV','MLTV','Width']].mean().round(1)
print('\nAverage values by NSP class:')
print(nsp_stats)

#SIMPLE INSIGHTS
insights = '''
KEY PATTERNS & INSIGHTS:

1️ CLASS IMBALANCE
* 75% records are NSP=1 (Normal)
* Only 20% suspicious (NSP=2), 5% pathologic (NSP=3+)

2️ RISK INDICATORS (High correlation with NSP)
* ASTV (0.52): Higher abnormal short-term variability = higher risk
* ALTV (0.46): Higher abnormal long-term variability = higher risk
* MLTV (0.40): Higher mean long-term variability = higher risk
* Width (0.09): Wider FHR patterns = higher risk

3 HIGHER RISK = MORE VARIABILITY
Higher NSP classes have:
* Higher ASTV (47-->64)
* Higher ALTV (10-->25)
* Higher MLTV (8-->11)
* Wider patterns (70-->95)

4 BUSINESS IMPLICATION
These 4 features (ASTV, ALTV, MLTV, Width) can predict fetal distress
* Focus monitoring on patients with high variability readings
'''
print(insights)

plt.figure(figsize=(10,8))
plt.scatter(df['ASTV'], df['NSP'], alpha=0.5, c=df['NSP_cat'])
plt.xlabel('ASTV (Abnormal Short-term Variability)')
plt.ylabel('NSP (Fetal Risk Score)')
plt.title('Pattern: Higher ASTV-Higher Risk (r=0.52)')
plt.axhline(y=1, color='red', linestyle='--', label='Risk threshold')
plt.legend()
plt.show()

print('Clear pattern: ASTV>60 often indicates higher risk')

"""5.	Conclusion:
●	Summarize the key insights and patterns discovered through your exploratory analysis.
●	Discuss how these findings could impact decision-making or further analyses.

"""

print('*** EDA1 SUMMARY ***')

# Final key metrics
print(f'Dataset: {df.shape[0]} records, {df.shape[1]} features')
print(f'Target: NSP (fetal health) - {len(df['NSP_cat'].unique())} classes')
print(f'Missing values: {df.isnull().sum().sum()} (all cleaned)')

# Quick final insights table
final_table = pd.DataFrame({
    'Finding': ['Class imbalance', 'Top risk predictors', 'Data quality', 'Next steps'],
    'Details': [
        '75% Normal, 20% Suspicious, 5% Pathologic',
        'ASTV(0.52), ALTV(0.46), MLTV(0.40)',
        '21 missing values filled, outliers noted',
        'Build ML model using ASTV+ALTV+MLTV+Width'
    ]
})
print("\nFinal Summary Table:")
print(final_table)

recommendations = '''
## KEY CONCLUSIONS & RECOMMENDATIONS

### 1. Dataset Overview
*Cleaned 2126 patient records with 14 fetal monitoring features
* NSP target: 1=Normal, 2=Suspicious, 3+=Pathologic

### 2. Major Findings
* 'HIGH RISK SIGNALS': ASTV, ALTV, MLTV, Width strongly predict fetal distress
* 'CLASS IMBALANCE': 75% normal cases --> need special ML handling
* 'VARIABILITY = DANGER': Higher variability --> higher NSP risk

### 3. Next Steps
1. Build classification model (Random Forest/XGBoost)
2. Handle class imbalance (SMOTE/undersampling)
3. Deploy real-time monitoring system

 Dataset is 'ML-ready' for fetal health prediction
'''
print(recommendations)

