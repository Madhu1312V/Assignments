# -*- coding: utf-8 -*-
"""Assign(15).ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/10uTyYuRyN2jYgQpO7dhY27jrCczirGl0
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import warnings
warnings.filterwarnings('ignore')
from sklearn.preprocessing import StandardScaler
from sklearn.cluster import KMeans, DBSCAN
from sklearn.metrics import silhouette_score
from sklearn.decomposition import PCA

"""Data Preprocessing:"""

# 1. Load Dataset
df = pd.read_excel('EastWestAirlines.xlsx', sheet_name='data')

# 2. Drop non-informative columns (like ID#)
df_clean = df.drop(['ID#'], axis=1)

# 3. Handle Missing Values (if any)
df_clean = df_clean.dropna()

# 4. Feature Scaling
# This is crucial so that columns with large numbers (like Balance) don't dominate.
scaler = StandardScaler()
df_scaled = scaler.fit_transform(df_clean)

"""Implementing Clustering Algorithms:"""

# Finding optimal K using Elbow Method
wcss = []
for i in range(1, 11):
    kmeans = KMeans(n_clusters=i, init='k-means++', random_state=42)
    kmeans.fit(df_scaled)
    wcss.append(kmeans.inertia_)

plt.plot(range(1, 11), wcss, marker='o')
plt.title('Elbow Method')
plt.xlabel('Number of clusters')
plt.ylabel('WCSS')
plt.show()

# Based on the elbow, let's assume K=5
kmeans = KMeans(n_clusters=5, init='k-means++', random_state=42)
df['KMeans_Labels'] = kmeans.fit_predict(df_scaled)

"""DBSCAN Clustering:"""

# Implementing DBSCAN
# eps: maximum distance between two samples to be considered neighbors
# min_samples: number of samples in a neighborhood for a point to be a core point
dbscan = DBSCAN(eps=0.5, min_samples=5)
df['DBSCAN_Labels'] = dbscan.fit_predict(df_scaled)

# Note: -1 in labels represents noise/outliers.

"""Cluster Analysis & Interpretation:"""

# Analyze the mean values of each cluster to see characteristics
analysis = df.groupby('KMeans_Labels').mean()
print(analysis)

# Insight: Some clusters might represent "Frequent Flyers" (high Balance/Bonus_miles),
# while others might be "New Customers" (low Days_since_enroll).

"""Visualization:"""

# 1. Reduce dimensions to 2D for visualization
pca = PCA(n_components=2)
pca_data = pca.fit_transform(df_scaled)
pca_df = pd.DataFrame(pca_data, columns=['PC1', 'PC2'])

# Add the cluster labels from our previous models
pca_df['KMeans_Labels'] = kmeans.labels_
pca_df['DBSCAN_Labels'] = dbscan.labels_

# 2. Plotting K-Means Clusters
plt.figure(figsize=(10, 6))
sns.scatterplot(x='PC1', y='PC2', hue='KMeans_Labels', palette='viridis', data=pca_df)
plt.title('Visualizing K-Means Clusters (K=5)')
plt.savefig('kmeans_clusters.png')

# 3. Plotting DBSCAN Clusters
plt.figure(figsize=(10, 6))
sns.scatterplot(x='PC1', y='PC2', hue='DBSCAN_Labels', palette='tab10', data=pca_df)
plt.title('Visualizing DBSCAN Clusters & Noise (-1)')
plt.savefig('dbscan_clusters.png')

"""Evaluation and Performance Metrics:"""

# Silhouette Score for K-Means
sil_kmeans = silhouette_score(df_scaled, df['KMeans_Labels'])
print(f"K-Means Silhouette Score: {sil_kmeans:.2f}")

# Silhouette Score for DBSCAN (excluding noise)
# We filter out noise points (-1) for a clearer score
mask = df['DBSCAN_Labels'] != -1
if any(mask):
    sil_dbscan = silhouette_score(df_scaled[mask], df.loc[mask, 'DBSCAN_Labels'])
    print(f"DBSCAN Silhouette Score: {sil_dbscan:.2f}")
else:
    print("DBSCAN could not find valid clusters with current parameters.")

"""Evaluation and Performance Metrics:

The quality of clustering is measured using internal evaluation metrics like the Silhouette Score.

- Silhouette Score: Measures how similar an object is to its own cluster compared to other clusters. A score closer to 1 is better.

- Inertia (WCSS): Specific to K-Means, it measures how tightly packed the clusters are.

Silhouette Score:

The Silhouette Score measures how similar a data point is to its own cluster (cohesion) compared to other clusters (separation).

Range: -1 to +1.

Interpretation:

Near +1: Excellent separation; the clusters are well-defined and far apart.

Near 0: Significant overlap between clusters.

Near -1: Data points might be assigned to the wrong clusters.

Results for EastWestAirlines:

Algorithm,Configuration,Silhouette Score,Interpretation

- K-Means, K=2, 0.3254, "Best score for K-Means, suggesting two broad groups."
- K-Means, K=5, 0.2005, "Lower score, indicating some overlap as we create finer segments."
- DBSCAN, Eps=1.5, 0.2537,"Fairly good, showing dense regions are being captured."
- DBSCAN, Eps=0.5, -0.0783, Poor performance; too many small clusters with high noise.

WCSS (Within-Cluster Sum of Squares) - For K-Means

Also known as Inertia, this measures how tightly packed the points are inside each cluster.

Goal: A lower WCSS is better, but it always decreases as you add more clusters.

The Elbow Rule: We look for the "K" where the rate of decrease in WCSS slows down (the "elbow"). In this dataset, the elbow typically appears between K=3 and K=5.

Summary of Steps

- Preprocessing: Remove the ID# column and scale the data using StandardScaler.
- K-Means: Use the Elbow curve to pick K and implement using scikit-learn.
- DBSCAN: Experiment with eps and min_samples to identify dense clusters and outliers.
- Analysis: Interpret cluster centers to identify customer segments like high-earning vs. low-spending travelers.
- Metrics: Use the silhouette_score to validate the quality of the clusters formed.

K-Means is better if the goal is to segment every single customer into a specific group for a marketing campaign.

DBSCAN is better if the goal is to identify common "dense" behaviors and separate out "outlier" customers who might need personalized high-end concierge services.

For the EastWestAirlines dataset, a K-Means with $K=5$ offers a balanced trade-off between a decent Silhouette Score (~0.20) and providing enough distinct segments (5 groups) for a marketing team to target effectively.
"""

