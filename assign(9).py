# -*- coding: utf-8 -*-
"""Assign(9).ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1HGBfO1jXT8JyJZWHV4UAds2XU_SJHtj0
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import warnings
warnings.filterwarnings('ignore')
from sklearn.impute import SimpleImputer
from sklearn.preprocessing import StandardScaler,MinMaxScaler,LabelEncoder,OneHotEncoder

df=pd.read_csv('adult_with_headers (1).csv')

df.shape

df.head()

df.info()

"""Missing values count"""

df.isnull().sum()

df.isnull().sum().sum()/len(df)*100

"""- here there is no missing values
- if missing values is <5% we can drop them
- else >5% replace with mean/median/mode respectively

Handle missing values

- Separate numerical and categorical columns
"""

num_imputer = SimpleImputer(strategy="median")
cat_imputer = SimpleImputer(strategy="most_frequent")

num_cols = df.select_dtypes(include=["int64", "float64"]).columns
cat_cols = df.select_dtypes(include=["object"]).columns

df[num_cols] = num_imputer.fit_transform(df[num_cols])
df[cat_cols] = cat_imputer.fit_transform(df[cat_cols])

"""Scaling numerical features"""

std_sca=StandardScaler()
min_max_sca=MinMaxScaler()

df[num_cols]=std_sca.fit_transform(df[num_cols])
df[num_cols]=min_max_sca.fit_transform(df[num_cols])

"""- Use StandardScaler when your model assumes normality or relies on variance.
- StandardScaler range is in between (-3,3)
- Use MinMaxScaler when working with neural networks or bounded features.
- MinMaxScaler range is in between (0,1) or (-1,1)
- Sometimes, you even combine them — eg: StandardScaler for continuous features, MinMaxScaler for bounded ones.

Encoding Techniques :

Encoding categorical features

One-Hot Encoding for categorical variables with <5 categories
"""

one_hot_cols = [col for col in cat_cols if df[col].nunique() < 5]
label_cols = [col for col in cat_cols if df[col].nunique() >= 5]

# One-Hot Encoding
df_encoded = pd.get_dummies(df, columns=one_hot_cols)
print(df_encoded.head())

# Label Encoding
for col in label_cols:
    le = LabelEncoder()
    df_encoded[col] = le.fit_transform(df_encoded[col])
print(df_encoded.head())

"""One-Hot Encoding (OHE) :

Pros :

- No ordinal assumption: Treats categories as independent, avoiding false numerical relationships (eg: "red" != "blue").
- Works well with linear models: Algorithms like Logistic Regression or Linear Regression benefit because categorical values are represented as binary flags.
- Interpretability: Easy to understand — each column directly represents a category.

Cons :

- High dimensionality: If a feature has many categories (eg:"city"), OHE creates many columns -> sparse, memory-heavy.
- Curse of dimensionality: Can slow down training and reduce performance for models sensitive to feature count.
- Not ideal for tree-based models: Decision trees can handle categorical splits without needing OHE, so it may add unnecessary complexity.

Label Encoding (LE) :

Pros :

- Compact representation: Converts categories into integers, keeping dataset size small.
- Efficient for tree-based models: Decision Trees, Random Forests, and XGBoost can naturally handle integer-coded categories.
- Fast to apply: Simple transformation, no explosion of features.

Cons :

- Imposes artificial order: Assigns numbers (eg: "red"=1, "blue"=2, "green"=3), which may mislead algorithms into thinking categories have hierarchy.
- Problematic for linear models: Algorithms like Logistic Regression may interpret encoded values as continuous, leading to biased results.
- Less interpretable: Harder to directly understand compared to OHE.

Overall :

- Use OHE for small categorical features.
- Use LE for large categorical features, especially with tree-based models.

Feature Engineering :
"""

df_encoded["age_bucket"] = pd.cut(df_encoded["age"], bins=[0,25,45,65,100], labels=["young","adult","mid_age","senior"])
df_encoded["capital_diff"] = df_encoded["capital_gain"] - df_encoded["capital_loss"]

print(df_encoded["age_bucket"],df_encoded["capital_diff"])

"""- age_bucket -> groups ages
 into categories (captures non-linear relationships).

- Income often correlates with age, but not linearly.

- By grouping ages into buckets, the model captures non-linear relationships between age and income more effectively than raw age values.

- capital_diff -> net capital gain/loss (more meaningful than separate columns).

- Hours worked per week strongly influence income.

- Categorizing into ranges helps the model distinguish between part-time, full-time, and overtime workers without assuming linearity.

Summary

- Age Bucket -> captures career stage effects.
- Capital Difference -> summarizes financial gains/losses into one meaningful metric.
- Work Hours Category (optional) -> highlights work intensity patterns.

Transformation of skewed feature
"""

# log transform 'fnlwgt' if skewed
df_encoded["fnlwgt_log"] = np.log1p(df_encoded["fnlwgt"])

print(df_encoded.head())

"""- log1p-reduces skewness in highly skewed numerical features."""

