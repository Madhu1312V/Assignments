# -*- coding: utf-8 -*-
"""Assign(12).ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1h-t1ZNiTK-9u-9Zb7oc9wzfSuHrB4-vw
"""

import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import classification_report, confusion_matrix
from sklearn.ensemble import BaggingClassifier, AdaBoostClassifier
from sklearn.tree import DecisionTreeClassifier

"""1. Exploratory Data Analysis (EDA):"""

# Load the dataset
df = pd.read_excel("glass.xlsx", sheet_name="glass")

# Display the first few rows
print(df.head())

# Basic info
print(df.info())

# Summary statistics
print(df.describe())

# Check for missing values
print(df.isnull().sum())

print(df.dtypes)

"""2: Data Visualization:"""

# Convert all columns except 'Type' to numeric
for col in df.columns:
    if col != 'Type':
        df[col] = pd.to_numeric(df[col], errors='coerce')

# Check again
print(df.dtypes)

print(df.columns)

# Histograms
df.hist(figsize=(12, 10))
plt.tight_layout()
plt.show()

# Boxplots
plt.figure(figsize=(12, 6))
sns.boxplot(data=df.drop("Type", axis=1))
plt.title("Boxplot of Features")
plt.show()

# Pairplot
sns.pairplot(df, hue="Type")
plt.show()

"""3: Data Preprocessing"""

# Features and target
X = df.drop("Type", axis=1)
y = df["Type"]

# Feature scaling
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# Train-test split
X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.3, random_state=42, stratify=y)

"""4: Random Forest Model Implementation"""

# Train model
rf = RandomForestClassifier(random_state=42)
rf.fit(X_train, y_train)

# Predict
y_pred = rf.predict(X_test)

# Evaluation
print(confusion_matrix(y_test, y_pred))
print(classification_report(y_test, y_pred))

"""5: Bagging and Boosting Methods"""

# Bagging Results
bag_model = BaggingClassifier(estimator=DecisionTreeClassifier(), n_estimators=100, random_state=42)
bag_model.fit(X_train, y_train)
bag_pred = bag_model.predict(X_test)

print("Bagging Results:")
print(classification_report(y_test, bag_pred))

# Boosting Results
boost_model = AdaBoostClassifier(n_estimators=100, random_state=42)
boost_model.fit(X_train, y_train)
boost_pred = boost_model.predict(X_test)

print("Boosting Results:")
print(classification_report(y_test, boost_pred))

"""Apply the Bagging and Boosting methods and compare the results.

Bagging vs Boosting

- Bagging is 82% accuracy, Reduces variance, stable predictions,Less adaptive to hard cases
- Boosting is 85% ccuracy, Learns complex patterns, higher accuracy, Sensitive to noise, risk of overfit


- Bagging and Boosting are both ensemble methods that improve model performance, but they differ in how they train models and handle errors. Bagging reduces variance by training models independently in parallel, while Boosting reduces bias by training models sequentially to correct previous mistakes.
- Random Forest is essentially Bagging + random feature selection.
- Bagging is safer when data is noisy.
- Boosting often gives better accuracy but requires tuning (learning rate, n_estimators).
- For imbalanced classes, Boosting can help by focusing on minority classes.

Additional Notes:

1. Explain Bagging and Boosting methods. How is it different from each other.

Bagging

How it works:
- Creates multiple subsets of the training data using random sampling with replacement.
- Trains a base model (often Decision Trees) on each subset independently.
- Final prediction is made by averaging (regression) or majority voting (classification).
- Strengths:
- Reduces variance (less overfitting).
- Works well with unstable learners like decision trees.
- Example: Random Forest is a Bagging method with added random feature selection.

 Boosting

How it works:
- Models are trained sequentially. Each new model focuses on correcting the errors of the previous one.
- Misclassified samples get higher weights so the next learner pays more attention to them.
- Final prediction is a weighted combination of all models.
- Strengths:
- Reduces bias (learns complex patterns).
- Often achieves higher accuracy than Bagging.
- Weakness:
- More sensitive to noise and outliers.
- Examples: AdaBoost, Gradient Boosting, XGBoost.

 Key Differences:

 Bagging

 - Training Style:Parallel (independent models)
 - Focus:Reduce variance
 - Risk of Overfitting:Lower
 - Example:Random Forest

 Boosting

- Training Style:Sequential (each model corrects errors)
- Focus:Reduce bias
- Risk of Overfitting:Higher (if not tuned)
- Example:AdaBoost, Gradient Boosting

2. Explain how to handle imbalance in the data.

Handling Imbalanced Data:
- Use stratify=y in train-test split.
- Try class_weight='balanced' in classifiers.

Strategies:

1.Resampling Techniques :
- Oversampling minority class (e.g., SMOTE â€“ Synthetic Minority Oversampling Technique).
- Undersampling majority class to balance distribution.

2.Class Weights :
- Many classifiers (including Random Forest, Bagging, Boosting) allow class_weight='balanced'.
- This penalizes misclassification of minority classes more heavily.

3.Evaluation Metrics :
- Use metrics beyond accuracy: precision, recall, F1-score, ROC-AUC.
- Accuracy alone can be misleading in imbalanced datasets.

4.Ensemble Methods :
- Boosting often helps minority classes because it focuses on misclassified samples.
- Bagging stabilizes predictions but may still favor majority classes unless balanced sampling is applied.


 Summary:
- Bagging = variance reduction, safer, stable.
- Boosting = bias reduction, more powerful but riskier.
- For imbalance: use resampling, class weights, and better metrics to ensure fair evaluation.
"""

